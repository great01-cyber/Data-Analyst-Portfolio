##Abstract
Currently, breast cancer detection utilises machine learning algorithms. However, these models are often referred to as "black boxes," which obscure their internal processes from users. This lack of transparency makes their outputs difficult for doctors to interpret. In response, this study explores and incorporates explainability facilities to machine learning model used for breast cancer classification. Data sourced from the University of California, Irvine Machine Learning Repository was used to create a machine learning model that predicts the occurrence of breast cancer, with Shapley Additive exPlanations (SHAP) and Local Interpretable Model-Agnostic Explanations (LIME) employed to interpret the model's predictions. The dataset used in the training process is a numeric dataset, and no images was used. Also, this research took a step further to evaluate the explainability produced by LIME and SHAP, by conducting a survey which was distributed through Qualtrics. The explainability was assessed based on four criteria: understandability, sufficiency, trustworthiness, and satisfaction. Additionally, both models were compared against each other to evaluate consistency, and a degree of consistency was found between LIME and SHAP. Also, this research identified a crucial gap, which is the lack of breast cancer diagnosis decision support systems. In response, an extensive research was conducted to identify possible risk factors of breast cancer, and a clinical breast cancer decision support system was designed, using a package in python called “Tkinter”, which could be further developed in the future. Hence, this thesis demonstrates that explainable AI can provide clear insights into how models generate their predictions, which is essential for increasing trust and acceptance of advanced ML methods in oncology and healthcare.

